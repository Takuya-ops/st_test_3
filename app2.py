import streamlit as st
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="AIå°‚é–€å®¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ", page_icon="ğŸ¤–", layout="centered"
)

st.title("ğŸ¤– AIå°‚é–€å®¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ")

st.write("### ã‚¢ãƒ—ãƒªã®æ¦‚è¦")
st.write(
    """
ã“ã®ã‚¢ãƒ—ãƒªã§ã¯ã€æ§˜ã€…ãªåˆ†é‡ã®å°‚é–€å®¶AIã«è³ªå•ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
å°‚é–€å®¶ã‚’é¸æŠã—ã¦è³ªå•ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ãã®åˆ†é‡ã®å°‚é–€çš„ãªè¦–ç‚¹ã‹ã‚‰å›ç­”ã‚’å¾—ã‚‰ã‚Œã¾ã™ã€‚
"""
)

st.divider()

# å°‚é–€å®¶ã®å®šç¾©
EXPERTS = {
    "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¬›å¸«": "ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¬›å¸«ã§ã™ã€‚åˆå¿ƒè€…ã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãã€å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’äº¤ãˆãªãŒã‚‰ä¸å¯§ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
    "ãƒ“ã‚¸ãƒã‚¹ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ": "ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªãƒ“ã‚¸ãƒã‚¹ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚æˆ¦ç•¥çš„ãªè¦–ç‚¹ã‹ã‚‰ã€å®Ÿè·µçš„ã§å…·ä½“çš„ãªãƒ“ã‚¸ãƒã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
    "å¥åº·ãƒ»ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼": "ã‚ãªãŸã¯å¥åº·ã¨ãƒ•ã‚£ãƒƒãƒˆãƒã‚¹ã®å°‚é–€å®¶ã§ã™ã€‚ç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ã„ãŸå¥åº·çš„ãªãƒ©ã‚¤ãƒ•ã‚¹ã‚¿ã‚¤ãƒ«ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
    "ã‚­ãƒ£ãƒªã‚¢ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼": "ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªã‚­ãƒ£ãƒªã‚¢ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã™ã€‚ã‚­ãƒ£ãƒªã‚¢å½¢æˆã‚„è»¢è·ã€ã‚¹ã‚­ãƒ«ã‚¢ãƒƒãƒ—ã«ã¤ã„ã¦ã€å…·ä½“çš„ã§å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚",
}


def get_llm_response(user_input: str, selected_expert: str) -> str:
    """
    å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨é¸æŠã•ã‚ŒãŸå°‚é–€å®¶ã«åŸºã¥ã„ã¦LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹

    Args:
        user_input (str): ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ãƒ†ã‚­ã‚¹ãƒˆ
        selected_expert (str): é¸æŠã•ã‚ŒãŸå°‚é–€å®¶ã®ç¨®é¡

    Returns:
        str: LLMã‹ã‚‰ã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
    """
    try:
        # ChatOpenAIãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å–å¾—
        system_message = EXPERTS[selected_expert]

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä½œæˆ
        prompt = ChatPromptTemplate.from_messages(
            [("system", system_message), ("human", "{input}")]
        )

        # ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆã¨å®Ÿè¡Œ
        chain = prompt | llm
        response = chain.invoke({"input": user_input})

        return response.content

    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"


st.write("### ä½¿ã„æ–¹")
st.write(
    """
1. ç›¸è«‡ã—ãŸã„å°‚é–€å®¶ã‚’é¸æŠã—ã¦ãã ã•ã„
2. è³ªå•ã‚„ç›¸è«‡å†…å®¹ã‚’å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ ã«è¨˜å…¥ã—ã¦ãã ã•ã„
3. ã€Œè³ªå•ã™ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„
4. AIã‹ã‚‰ã®å›ç­”ãŒè¡¨ç¤ºã•ã‚Œã¾ã™
"""
)

st.divider()

# å°‚é–€å®¶ã®é¸æŠ(ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³)
selected_expert = st.radio("ç›¸è«‡ã—ãŸã„å°‚é–€å®¶ã‚’é¸æŠã—ã¦ãã ã•ã„", list(EXPERTS.keys()))

# è³ªå•å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
user_input = st.text_area(
    label="è³ªå•ã‚„ç›¸è«‡å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
    placeholder="ä¾‹: Pythonã§ãƒªã‚¹ãƒˆã¨è¾æ›¸ã®é•ã„ã‚’æ•™ãˆã¦ãã ã•ã„",
    height=150,
)

# è³ªå•ãƒœã‚¿ãƒ³
if st.button("è³ªå•ã™ã‚‹", type="primary"):
    if user_input:
        with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
            # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—
            response = get_llm_response(user_input, selected_expert)

            st.divider()

            # å›ç­”ã®è¡¨ç¤º
            st.write("### ğŸ’¡ å›ç­”")
            st.write(f"**å°‚é–€å®¶:** {selected_expert}")
            st.write(response)
    else:
        st.error("è³ªå•ã‚’å…¥åŠ›ã—ã¦ã‹ã‚‰ã€Œè³ªå•ã™ã‚‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

# ãƒ•ãƒƒã‚¿ãƒ¼
st.divider()
st.caption("Powered by OpenAI GPT-4o-mini & LangChain")
